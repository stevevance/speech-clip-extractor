# Speech Clip Extractor Skill

Use this skill when the user wants to extract highlight clips from a speech or presentation video. It applies when:

- The user has a video file (`.mov`, `.mp4`, etc.) of a speech, press conference, or public address
- The user has a VTT subtitle file (e.g. auto-generated by Vimeo or YouTube)
- The user wants to identify quotable moments and export them as short clips
- The user needs both horizontal (16:9) and vertical (9:16) versions for different platforms

---

## Workflow

### Step 0 — Generate subtitles (if no VTT file exists)

**Preferred tool: `mlx-whisper`** — runs natively on Apple Silicon (arm64) using the Neural Engine/GPU. Much faster than OpenAI Whisper on CPU.

> **Requirement**: mlx-whisper only installs on arm64 Python. Use Homebrew Python, not Anaconda (which runs as x86_64 under Rosetta).

Install into a temporary venv:
```bash
/opt/homebrew/bin/python3.14 -m venv /tmp/mlx-venv
/tmp/mlx-venv/bin/pip install mlx-whisper
```

Run transcription:
```bash
/tmp/mlx-venv/bin/mlx_whisper "/path/to/video.mp4" \
  --model mlx-community/whisper-medium-mlx \
  --output-format vtt \
  --output-dir /path/to/output/dir
```

The VTT file will be saved alongside the video with the same base filename.

---

### Step 1 — Read the VTT file

Read the `.vtt` subtitle file to understand the full transcript with timestamps.

```
Read: /path/to/captions.vtt
```

### Step 2 — Identify highlight moments

Analyze the transcript and extract the most notable quotes. Present them to the user as a table:

| Timestamp | Quote | Why It's Notable |
|-----------|-------|-----------------|
| 0:14 | "Everything is just too damned expensive." | Short, punchy, quotable |
| 1:05 | "Rules with shameful roots that go back to redlining." | Politically charged, specific |

Look for:
- Short, self-contained soundbites (under 20 seconds)
- Surprising or strong language
- Specific data points or statistics
- Clear problem/solution statements
- Rhetorical flourishes or memorable phrases

### Step 3 — Get the video file path and dimensions

Ask the user for the video file path if not provided. Then check dimensions:

```bash
ffprobe -v error -select_streams v:0 \
  -show_entries stream=width,height -of csv=p=0 \
  "/path/to/video.mov"
```

### Step 4 — Extract clips

For each clip, run horizontal and vertical versions in parallel as background tasks. Always include `-pix_fmt yuv420p -movflags +faststart` so output files are compatible with QuickTime and the macOS Photos app.

**Horizontal (full frame):**
```bash
ffmpeg -y -ss [START] -to [END] \
  -i "/path/to/video.mov" \
  -c:v libx264 -preset fast -crf 23 \
  -pix_fmt yuv420p -movflags +faststart \
  -c:a aac \
  output_horizontal.mp4
```

**Vertical (9:16 center crop from 1920×1080):**
```bash
ffmpeg -y -ss [START] -to [END] \
  -i "/path/to/video.mov" \
  -vf "crop=608:1080:656:0" \
  -c:v libx264 -preset fast -crf 23 \
  -pix_fmt yuv420p -movflags +faststart \
  -c:a aac \
  output_vertical.mp4
```

Use `run_in_background: true` and wait for all tasks before reporting results.

### Step 5 — Generate VTT subtitles for a clip (optional)

If the user wants a subtitle file for a clip, use this Python snippet to filter and re-zero the timestamps:

```python
import re

CLIP_START = 32.0  # seconds from original video
CLIP_END = 137.0

def ts_to_seconds(ts):
    h, m, s = ts.split(':')
    return int(h)*3600 + int(m)*60 + float(s)

def seconds_to_ts(s):
    h = int(s // 3600)
    m = int((s % 3600) // 60)
    sec = s % 60
    return f"{h:02d}:{m:02d}:{sec:06.3f}"

with open("captions.vtt") as f:
    content = f.read()

blocks = content.strip().split('\n\n')
output = ["WEBVTT", ""]
new_index = 1

for block in blocks:
    lines = block.strip().split('\n')
    if '-->' not in block:
        continue
    timing_line = next(l for l in lines if '-->' in l)
    text_lines = [l for l in lines if '-->' not in l and not l.strip().isdigit()]
    start_ts, end_ts = timing_line.split(' --> ')
    start_s = ts_to_seconds(start_ts.strip())
    end_s = ts_to_seconds(end_ts.strip())
    if start_s >= CLIP_START and end_s <= CLIP_END:
        new_start = seconds_to_ts(start_s - CLIP_START)
        new_end = seconds_to_ts(end_s - CLIP_START)
        output.append(str(new_index))
        output.append(f"{new_start} --> {new_end}")
        output.extend(text_lines)
        output.append("")
        new_index += 1

with open("clip_subtitles.vtt", 'w') as f:
    f.write('\n'.join(output))
```

---

## Vertical Crop Math

For source resolutions other than 1920×1080:

```
crop_width = source_height × (9 / 16)
x_offset   = (source_width − crop_width) / 2
ffmpeg filter: crop=crop_width:source_height:x_offset:0
```

| Source Resolution | Crop Filter |
|-------------------|-------------|
| 1920×1080 | `crop=608:1080:656:0` |
| 1280×720 | `crop=405:720:437:0` |
| 3840×2160 | `crop=1215:2160:1312:0` |

---

## Tips

- Run horizontal and vertical extractions in parallel using `run_in_background: true`
- For combined segments (multiple clips in one), use a single `-ss` / `-to` range covering all clips
- Always confirm output file paths with the user before starting if there are many clips
- Offer to generate a VTT subtitle file for any clip that is longer than ~30 seconds
